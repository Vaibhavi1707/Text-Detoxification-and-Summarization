{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abstract\n\nThis notebook holds an implementation of the TextRank algorithm to summarize the detoxified text obtained from the notebook on Text Detoxification with BERT. The TextRank algorithm is an unsupervised learning technique for summarising text. It uses Google's page rank algorithm to assign importance to the sentences in the text which are connected through a Markov Chain with transition probabilities same as the similarity score of the source and the destination sentence. We then take the most important sentences based on the importance assigned to construct the summary of the input text. We use the text of the CNN Dailymail dataset in order to test our methods. We use the GloVe embeddings to vectorize the sentence and the ROUGE score as a metric to evaluate the summarisation.","metadata":{}},{"cell_type":"markdown","source":"# Imports and Installations\n\nWe used the numpy and pandas to read ans store the CNN Dailymail dataset, nltk library for text preprocessing, the sklearn library to calculate the cosine similarity between 2 vectorized sentences.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nfrom torch.nn import functional as func\nimport torch\nimport pickle\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt') # one time execution\nstop_words = stopwords.words('english')\n\nimport re\nimport os\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport networkx as nx\n\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-14T07:52:53.721266Z","iopub.execute_input":"2022-05-14T07:52:53.721931Z","iopub.status.idle":"2022-05-14T07:56:10.276607Z","shell.execute_reply.started":"2022-05-14T07:52:53.721820Z","shell.execute_reply":"2022-05-14T07:56:10.275530Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c2648b1e7e4b1e8c457f0418f01764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0132c0480f7048b39d0973255bf5a59e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33954cf2d2e8461591d1428cedc60ca6"}},"metadata":{}},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n--2022-05-14 07:53:06--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2022-05-14 07:53:06--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2022-05-14 07:53:06--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘glove.6B.zip’\n\nglove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 40s  \n\n2022-05-14 07:55:46 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n\nArchive:  glove.6B.zip\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt       \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing the CNN Daily Mail Dataset\n\nThe CNN Daily Mail Dataset is a popular dataset and is noted to perform well for extractive as well as summarisation with a parallel dataset. In this notebook, we would be using the TextRank Algorithm, which is an extractive algorithm to summarise the sentences. ","metadata":{}},{"cell_type":"code","source":"TRAIN_CSV = '../input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv'","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:10.279037Z","iopub.execute_input":"2022-05-14T07:56:10.279299Z","iopub.status.idle":"2022-05-14T07:56:10.283931Z","shell.execute_reply.started":"2022-05-14T07:56:10.279265Z","shell.execute_reply":"2022-05-14T07:56:10.283021Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:10.285075Z","iopub.execute_input":"2022-05-14T07:56:10.285266Z","iopub.status.idle":"2022-05-14T07:56:37.614397Z","shell.execute_reply.started":"2022-05-14T07:56:10.285242Z","shell.execute_reply":"2022-05-14T07:56:37.613470Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                         id  \\\n0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n2  00027e965c8264c35cc1bc55556db388da82b07f   \n3  0002c17436637c4fe1837c935c04de47adb18e9a   \n4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n\n                                             article  \\\n0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n1  (CNN) -- Ralph Mata was an internal affairs li...   \n2  A drunk driver who killed a young woman in a h...   \n3  (CNN) -- With a breezy sweep of his pen Presid...   \n4  Fleetwood are the only team still to have a 10...   \n\n                                          highlights  \n0  Bishop John Folda, of North Dakota, is taking ...  \n1  Criminal complaint: Cop used his role to help ...  \n2  Craig Eccleston-Todd, 27, had drunk at least t...  \n3  Nina dos Santos says Europe must be ready to a...  \n4  Fleetwood top of League One after 2-0 win at S...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>article</th>\n      <th>highlights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n      <td>Criminal complaint: Cop used his role to help ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n      <td>A drunk driver who killed a young woman in a h...</td>\n      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n      <td>Nina dos Santos says Europe must be ready to a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n      <td>Fleetwood are the only team still to have a 10...</td>\n      <td>Fleetwood top of League One after 2-0 win at S...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Importing the TF-IDF Vectorizer","metadata":{}},{"cell_type":"code","source":"vectorizer = pickle.load(open(\"/kaggle/input/tfidf-vectorizer/vectorizer.pickle\", \"rb\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:37.616333Z","iopub.execute_input":"2022-05-14T07:56:37.616679Z","iopub.status.idle":"2022-05-14T07:56:37.638566Z","shell.execute_reply.started":"2022-05-14T07:56:37.616627Z","shell.execute_reply":"2022-05-14T07:56:37.637769Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing the BoW Toxicity Classifier","metadata":{}},{"cell_type":"code","source":"BoWClf = torch.load('/kaggle/input/toxicityclassifier/BoWClf.pt')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:37.639936Z","iopub.execute_input":"2022-05-14T07:56:37.640315Z","iopub.status.idle":"2022-05-14T07:56:37.653886Z","shell.execute_reply.started":"2022-05-14T07:56:37.640268Z","shell.execute_reply":"2022-05-14T07:56:37.652731Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing Word-Toxicity Scores","metadata":{}},{"cell_type":"code","source":"wordtoxicities_df = pd.read_csv('/kaggle/input/wordtoxicityscores/wordtoxicities.csv', index_col = 0)\nwordtoxicities_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:37.655193Z","iopub.execute_input":"2022-05-14T07:56:37.655506Z","iopub.status.idle":"2022-05-14T07:56:37.700509Z","shell.execute_reply.started":"2022-05-14T07:56:37.655464Z","shell.execute_reply":"2022-05-14T07:56:37.699676Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"             word  toxicity\n0     aaaaaaaaaah  0.155084\n1              ab -0.089550\n2             aba -0.160136\n3         abandon -0.060261\n4  abbasidumayyad -0.050418","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>toxicity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aaaaaaaaaah</td>\n      <td>0.155084</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ab</td>\n      <td>-0.089550</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aba</td>\n      <td>-0.160136</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abandon</td>\n      <td>-0.060261</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abbasidumayyad</td>\n      <td>-0.050418</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Importing the fine-tuned BERT Model","metadata":{}},{"cell_type":"code","source":"token_logits = torch.load('/kaggle/input/detoxifier/BERTLogits.pt')\ntoken_logits","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:37.701984Z","iopub.execute_input":"2022-05-14T07:56:37.702499Z","iopub.status.idle":"2022-05-14T07:56:38.278420Z","shell.execute_reply.started":"2022-05-14T07:56:37.702455Z","shell.execute_reply":"2022-05-14T07:56:38.277581Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([[[-2.4611e+00, -8.5963e+00, -8.7396e+00,  ..., -9.5399e+00,\n          -1.1249e+01, -5.7524e+00],\n         [-4.1351e+00, -7.5703e+00, -7.4469e+00,  ..., -6.2272e+00,\n          -7.3544e+00, -8.3536e+00],\n         [-1.4242e+00, -5.3903e+00, -5.3435e+00,  ..., -3.8646e+00,\n          -5.8753e+00, -6.3117e+00],\n         ...,\n         [ 1.8846e+01,  5.3556e+00,  4.9811e+00,  ...,  1.7870e+00,\n           1.6240e+00,  8.8052e-01],\n         [ 1.9111e+01,  5.5498e+00,  5.3283e+00,  ...,  2.7407e+00,\n           2.2640e+00,  1.0450e+00],\n         [ 1.8342e+01,  4.5627e+00,  4.3856e+00,  ...,  1.3594e+00,\n           1.4697e+00,  7.5490e-03]],\n\n        [[-4.4690e+00, -9.7690e+00, -9.9032e+00,  ..., -8.8216e+00,\n          -1.1228e+01, -5.3673e+00],\n         [-8.0420e-01, -6.5122e+00, -6.1049e+00,  ..., -7.9757e+00,\n          -7.6022e+00, -4.5425e+00],\n         [-2.1502e+00, -7.5105e+00, -7.3783e+00,  ..., -7.8517e+00,\n          -7.7302e+00, -4.9595e+00],\n         ...,\n         [ 1.9179e+01,  5.5162e+00,  5.3242e+00,  ...,  2.6898e+00,\n           1.9065e+00,  8.9789e-01],\n         [ 1.9540e+01,  5.9370e+00,  5.7696e+00,  ...,  2.9302e+00,\n           2.4973e+00,  6.6368e-01],\n         [ 1.9331e+01,  5.8599e+00,  5.5589e+00,  ...,  2.3010e+00,\n           2.2724e+00,  9.7092e-01]],\n\n        [[-4.4835e+00, -9.4408e+00, -9.5508e+00,  ..., -9.0385e+00,\n          -1.1637e+01, -7.5000e+00],\n         [-4.8528e+00, -8.8373e+00, -9.3894e+00,  ..., -9.9610e+00,\n          -9.3962e+00, -8.0600e+00],\n         [-3.2337e+00, -6.6952e+00, -6.9747e+00,  ..., -6.8095e+00,\n          -4.8587e+00, -6.9903e+00],\n         ...,\n         [ 1.8985e+01,  5.3140e+00,  5.0017e+00,  ...,  2.4343e+00,\n           1.6120e+00,  4.1101e-01],\n         [ 1.9255e+01,  5.5954e+00,  5.2852e+00,  ...,  2.4732e+00,\n           2.0515e+00,  1.2902e+00],\n         [ 1.9738e+01,  6.1919e+00,  5.9037e+00,  ...,  3.1969e+00,\n           2.7307e+00,  1.4566e+00]],\n\n        ...,\n\n        [[-6.3522e+00, -1.0864e+01, -1.0901e+01,  ..., -9.7651e+00,\n          -1.2571e+01, -7.7834e+00],\n         [-5.2532e+00, -9.3754e+00, -9.5414e+00,  ..., -1.1247e+01,\n          -8.9706e+00, -6.7296e+00],\n         [-6.5397e+00, -9.5912e+00, -9.9992e+00,  ..., -1.0438e+01,\n          -9.2057e+00, -1.0702e+01],\n         ...,\n         [ 1.9205e+01,  5.5607e+00,  5.3377e+00,  ...,  2.8021e+00,\n           2.1860e+00,  1.8721e+00],\n         [ 1.9177e+01,  5.6467e+00,  5.2568e+00,  ...,  2.5713e+00,\n           1.8930e+00,  8.1912e-01],\n         [ 1.8891e+01,  5.2793e+00,  5.0618e+00,  ...,  1.9945e+00,\n           1.9171e+00,  9.2928e-01]],\n\n        [[-4.7711e+00, -9.7573e+00, -1.0176e+01,  ..., -9.5885e+00,\n          -1.1693e+01, -6.9547e+00],\n         [-5.2568e+00, -9.1413e+00, -9.2591e+00,  ..., -8.5639e+00,\n          -8.5536e+00, -8.3371e+00],\n         [-9.0160e+00, -1.2235e+01, -1.2538e+01,  ..., -1.1535e+01,\n          -1.0385e+01, -7.9082e+00],\n         ...,\n         [ 1.8896e+01,  5.0796e+00,  4.9022e+00,  ...,  1.8305e+00,\n           1.8346e+00,  8.6754e-01],\n         [ 1.8856e+01,  5.1490e+00,  4.9713e+00,  ...,  1.9101e+00,\n           1.5100e+00,  5.5087e-01],\n         [ 1.9310e+01,  5.7163e+00,  5.4500e+00,  ...,  2.4815e+00,\n           2.3254e+00,  6.7677e-01]],\n\n        [[-2.7218e+00, -8.2856e+00, -8.5079e+00,  ..., -8.3233e+00,\n          -1.0320e+01, -5.1000e+00],\n         [-4.3075e+00, -7.5309e+00, -7.5909e+00,  ..., -6.7317e+00,\n          -9.1359e+00, -6.5906e+00],\n         [ 1.2568e-01, -4.5960e+00, -4.6203e+00,  ..., -3.8871e+00,\n          -5.9076e+00, -3.5771e+00],\n         ...,\n         [ 1.9024e+01,  5.4020e+00,  5.0801e+00,  ...,  2.1994e+00,\n           1.9522e+00,  1.2579e+00],\n         [ 1.8671e+01,  5.1173e+00,  4.8185e+00,  ...,  1.8111e+00,\n           1.9280e+00,  9.5914e-02],\n         [ 1.8789e+01,  5.1758e+00,  4.8407e+00,  ...,  1.6110e+00,\n           1.8846e+00,  7.7507e-01]]], requires_grad=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Word Embeddings\n\nWe use the GloVe word embeddings to tokenize the sentences. In order to test our method, we have taken the second article out of the given set of articles.","metadata":{}},{"cell_type":"code","source":"text = train_df['article'][1]\n\ndef extract_word_vectors():\n    word_embeddings = {}\n    f = open('glove.6B.100d.txt', encoding='utf-8')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        word_embeddings[word] = coefs\n    f.close()\n    return word_embeddings\n\nembeddings = extract_word_vectors()","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:38.279451Z","iopub.execute_input":"2022-05-14T07:56:38.279659Z","iopub.status.idle":"2022-05-14T07:56:50.492608Z","shell.execute_reply.started":"2022-05-14T07:56:38.279617Z","shell.execute_reply":"2022-05-14T07:56:50.491777Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Functions for Preprocessing and Summarising the Text\n\n## Preprocessing\n\n- Splitting into sentences\n- Removing punctuations, converting to lower case \n- Remove stop words\n\n## Summarising\n\n- Tokenizing each sentence and applying the word embeddings to each word of the sentence to convert the sentence into a list of word embeddings. \n- Calulating the Markov Chain Matrix by calculating the similarity between two vectorized sentences and computing the consine similarity between the 2 vectors.\n- Find sentence importance rankings using the page rank algorithm.\n\n\nWe lastly pick the 2/3 of the sentences from the list rankings and store them as the summary.","metadata":{}},{"cell_type":"code","source":"def split_into_sentences(text):\n    return sent_tokenize(text)\n\ndef remove_punctuations(sentences):\n    return pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\ndef convert2lower(sentences):\n    return [s.lower() for s in sentences]\n\ndef remove_stopwords(sen):\n    return \" \".join([i for i in sen.split() if i not in stop_words])\n\ndef vectorize_sentences(clean_sentences, embeddings):\n    sentence_vectors = []\n    for i in clean_sentences:\n        if len(i) != 0:\n            v = sum([embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n        else:\n            v = np.zeros((100,))\n        sentence_vectors.append(v)\n    return sentence_vectors\n\ndef calc_similarity_mat(sentences, sentence_vectors):\n    sim_mat = np.zeros([len(sentences), len(sentences)])\n    for i in range(len(sentences)):\n        for j in range(len(sentences)):\n            if i != j:\n                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), \n                                                  sentence_vectors[j].reshape(1,100))[0,0]\n    return sim_mat\n                \ndef find_sentence_rankings(sim_mat):\n    nx_graph = nx.from_numpy_array(sim_mat)\n    return nx.pagerank(nx_graph)\n\ndef summarize(text):\n    sentences = split_into_sentences(text)\n    clean_sentences = [remove_stopwords(sent) for sent in convert2lower(remove_punctuations(sentences))]\n    \n    sentence_vectors = vectorize_sentences(clean_sentences, embeddings)\n    \n    sim_mat = calc_similarity_mat(sentences, sentence_vectors)\n    sentence_scores = find_sentence_rankings(sim_mat)\n    ranked_sentences = sorted(((sentence_scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n    \n    return ' '.join(sent for score, sent in ranked_sentences[:int(0.66 * len(ranked_sentences))])","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:50.493931Z","iopub.execute_input":"2022-05-14T07:56:50.494295Z","iopub.status.idle":"2022-05-14T07:56:50.749454Z","shell.execute_reply.started":"2022-05-14T07:56:50.494268Z","shell.execute_reply":"2022-05-14T07:56:50.748342Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"summary = summarize(text)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:50.752107Z","iopub.execute_input":"2022-05-14T07:56:50.752366Z","iopub.status.idle":"2022-05-14T07:56:50.858848Z","shell.execute_reply.started":"2022-05-14T07:56:50.752336Z","shell.execute_reply":"2022-05-14T07:56:50.856705Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  \"\"\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluating the Summarisation\n\nWe evaluate the summarisation performed by using the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score. The candidate text is nothing but the summary obtained of the input text while we use the `highlights` column of the CNN DailyMail dataset for the reference.  ","metadata":{}},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:56:50.860287Z","iopub.execute_input":"2022-05-14T07:56:50.863068Z","iopub.status.idle":"2022-05-14T07:57:03.133812Z","shell.execute_reply.started":"2022-05-14T07:56:50.863019Z","shell.execute_reply":"2022-05-14T07:57:03.132751Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from rouge import Rouge\n\nreference = train_df['highlights'][1]\n\n\ndef calculate_rouge_score(cand, ref):\n    return Rouge().get_scores(summary, reference)\n\ncandidates = train_df['article'][1]\nrefs = train_df['highlights'][1]\nscores = calculate_rouge_score(summary, refs)\n\nprint(\"@@CANDIDATE@@\", summary)\nprint(\"@@REFERENCE@@\", refs)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:57:03.137220Z","iopub.execute_input":"2022-05-14T07:57:03.137481Z","iopub.status.idle":"2022-05-14T07:57:03.166129Z","shell.execute_reply.started":"2022-05-14T07:57:03.137453Z","shell.execute_reply":"2022-05-14T07:57:03.165552Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"@@CANDIDATE@@ A criminal complaint unsealed in U.S. District Court in New Jersey Tuesday accuses Mata, also known as \"The Milk Man,\" of using his role as a police officer to help the drug trafficking organization in exchange for money and gifts, including a Rolex watch. Outside the office, authorities allege that the 45-year-old longtime officer worked with a drug trafficking organization to help plan a murder plot and get guns. \"Ultimately, the (organization) decided not to move forward with the murder plot, but Mata still received a payment for setting up the meetings,\" federal prosecutors said in a statement. Mata has worked for the Miami-Dade Police Department since 1992, including directing investigations in Miami Gardens and working as a lieutenant in the K-9 unit at Miami International Airport, according to the complaint. In one instance, the complaint alleges, Mata arranged to pay two assassins to kill rival drug dealers. It was not immediately clear whether Mata has an attorney, and police officials could not be immediately reached for comment. Court documents released by investigators do not specify the name of the drug trafficking organization with which Mata allegedly conspired but says the organization has been importing narcotics from places such as Ecuador and the Dominican Republic by hiding them \"inside shipping containers containing pallets of produce, including bananas.\" The organization \"has been distributing narcotics in New Jersey and elsewhere,\" the complaint says. The complaint also alleges that Mata used his police badge to purchase weapons for drug traffickers. Mata, according to the complaint, then used contacts at the airport to transport the weapons in his carry-on luggage on trips from Miami to the Dominican Republic. The killers would pose as cops, pulling over their targets before shooting them, according to the complaint.\n@@REFERENCE@@ Criminal complaint: Cop used his role to help cocaine traffickers .\nRalph Mata, an internal affairs lieutenant, allegedly helped group get guns .\nHe also arranged to pay two assassins in a murder plot, a complaint alleges .\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Scores for unigram comparison\")\nprint(\"RECALL:\", scores[0]['rouge-1']['r'])\nprint(\"PRECISION:\", scores[0]['rouge-1']['p'])\nprint(\"F1-SCORE\", scores[0]['rouge-1']['f'])\nprint()\n\nprint(\"Scores for bigram comparison\")\nprint(\"RECALL:\", scores[0]['rouge-2']['r'])\nprint(\"PRECISION:\", scores[0]['rouge-2']['p'])\nprint(\"F1-SCORE\", scores[0]['rouge-2']['f'])\nprint()\n\nprint(\"Scores for Longest Common Subsequence (LCS) Comparison\")\nprint(\"RECALL:\", scores[0]['rouge-l']['r'])\nprint(\"PRECISION:\", scores[0]['rouge-l']['p'])\nprint(\"F1-SCORE\", scores[0]['rouge-l']['f'])","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:57:03.166996Z","iopub.execute_input":"2022-05-14T07:57:03.167193Z","iopub.status.idle":"2022-05-14T07:57:03.180934Z","shell.execute_reply.started":"2022-05-14T07:57:03.167168Z","shell.execute_reply":"2022-05-14T07:57:03.179969Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Scores for unigram comparison\nRECALL: 0.6666666666666666\nPRECISION: 0.12571428571428572\nF1-SCORE 0.21153845886880548\n\nScores for bigram comparison\nRECALL: 0.3235294117647059\nPRECISION: 0.04044117647058824\nF1-SCORE 0.07189542286129272\n\nScores for Longest Common Subsequence (LCS) Comparison\nRECALL: 0.6666666666666666\nPRECISION: 0.12571428571428572\nF1-SCORE 0.21153845886880548\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# References\n\n- https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n- https://medium.com/analytics-vidhya/sentence-extraction-using-textrank-algorithm-7f5c8fd568cd\n- https://www.tensorflow.org/datasets/catalog/cnn_dailymail\n- https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail\n- https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471","metadata":{}},{"cell_type":"markdown","source":"# Human Evaluation of Text Detoxification and Summarization","metadata":{}},{"cell_type":"markdown","source":"## Inputs","metadata":{}},{"cell_type":"code","source":"test1 = \"Why in the fucking world is chidamabaram wasting time in the CONG party meeting ?? Discuss and decide shit what the party need to do in order to sustain in india... or just wind-up and stay at home or join with BJP..\"\ntest2 = \"Neither of you guys has made any contribution to this Italian history article other than to shove your unhistorical unconstructive modern POV in my face. This is the reason why so many people get pissed off about the pedantry and idiocy and triviality of Wikipedia. Jesus. Get a fucking life.\"","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:57:03.182060Z","iopub.execute_input":"2022-05-14T07:57:03.182275Z","iopub.status.idle":"2022-05-14T07:57:03.186335Z","shell.execute_reply.started":"2022-05-14T07:57:03.182249Z","shell.execute_reply":"2022-05-14T07:57:03.185696Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## BERT Snippet","metadata":{}},{"cell_type":"code","source":"max_tokens = 64\n\nwordtoxicities_dict = {}\nfor i in range(len(wordtoxicities_df)):\n    wordtoxicities_dict[wordtoxicities_df['word'].tolist()[i]] = wordtoxicities_df['toxicity'].tolist()[i]\n    \nmax_word_toxicity = np.max(wordtoxicities_df.toxicity.tolist())\nmin_word_toxicity = np.min(wordtoxicities_df.toxicity.tolist())\navg_word_toxicity = np.mean(wordtoxicities_df.toxicity.tolist())\n\ndef give_mask(s): \n    #assigning toxicity scores to tokens known from BoW Toxicity Classifier\n    inp = tokenizer(s, return_tensors = 'pt', max_length = max_tokens, truncation = True, padding = 'max_length') \n    \n    tokens = []\n    for token in inp.input_ids[0]:\n        tokens.append(''.join(tokenizer.decode(token).split()))\n    \n    token_scores = []\n    \n    for token in tokens:\n        if token in wordtoxicities_dict:\n            token_scores.append(wordtoxicities_dict[token])\n        else:\n            token_scores.append(avg_word_toxicity)        \n    \n    #finding words to be nasked using an adaptive threshold\n    min_threshold = 0.2\n    \n    s_array = np.array([s]) \n    s_series = pd.Series(s_array)\n    s_vectorized = vectorizer.transform(s_series).todense()\n    \n    if(BoWClf.predict(s_vectorized)[0]):\n        scores = np.zeros(max_tokens)\n        scores[:len(token_scores)] = np.array(token_scores)\n        scores = torch.from_numpy(scores)\n\n        threshold = max(min_threshold, max(token_scores) / 2)\n\n        mask = (scores > threshold) * (inp.input_ids[0] != 101) * (inp.input_ids[0] != 102) * (inp.input_ids[0] != 0)\n    else:\n        mask = False * (inp.input_ids[0] != 101) * (inp.input_ids[0] != 102) * (inp.input_ids[0] != 0)\n    \n    #applying mask\n    selection = torch.flatten(mask.nonzero()).tolist()\n    inp.input_ids[0, selection] = 103\n    \n    return inp.input_ids[0]\n        \ndef detoxify(s):\n    s_masked = give_mask(s) #masking toxic comment\n    \n    #retrieving detoxified comment\n    \n    s_detoxified = []\n    for token in s_masked:\n        s_detoxified.append(''.join(tokenizer.decode(token).split()))\n\n    mask_token_indices = torch.where(s_masked == tokenizer.mask_token_id)[0]\n\n    softy = func.softmax(token_logits, dim = -1)\n    mask_token_logits = softy[0, mask_token_indices, :]\n    \n    #rescoring on the basis of toxicities\n    for i in range(len(mask_token_indices.tolist())):\n        mask_tokens =  torch.sort(mask_token_logits, dim = 1, stable = True).indices[i].tolist()\n        logits_tokens = torch.sort(mask_token_logits, dim = 1, stable = True).values[i].tolist()\n\n        better_mask_tokens = []\n        better_mask_token_logits = []\n        for j in range(len(logits_tokens)):\n            better_mask_tokens.append(mask_tokens[j])\n\n            token = ''.join(tokenizer.decode(mask_tokens[j]).split())\n            if token in wordtoxicities_dict:\n                better_mask_token_logits.append(logits_tokens[j] / (max_word_toxicity - min_word_toxicity + wordtoxicities_dict[token]))\n            else:\n                better_mask_token_logits.append(logits_tokens[j] / (max_word_toxicity - min_word_toxicity + avg_word_toxicity))\n\n        better_mask_tokens.sort(key = dict(zip(better_mask_tokens, better_mask_token_logits)).get)\n        best_mask_token = better_mask_tokens[-1: ]\n\n        s_detoxified[mask_token_indices[i]] = tokenizer.decode(best_mask_token)\n        \n    return ' '.join(word for word in s_detoxified[1:s_detoxified.index('[SEP]')]).replace(\" ##\", \"\")","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:57:03.187377Z","iopub.execute_input":"2022-05-14T07:57:03.187832Z","iopub.status.idle":"2022-05-14T07:57:08.995844Z","shell.execute_reply.started":"2022-05-14T07:57:03.187802Z","shell.execute_reply":"2022-05-14T07:57:08.994934Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Outputs","metadata":{}},{"cell_type":"code","source":"summarized_text1 = summarize(test1)\n\ndetoxified_text1 = detoxify(test1)\n\nsummarized_detoxified_text1 = summarize(detoxified_text1)\n\nprint(test1)\nprint()\nprint(summarized_text1)\nprint()\nprint(summarized_detoxified_text1)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:57:08.996971Z","iopub.execute_input":"2022-05-14T07:57:08.997175Z","iopub.status.idle":"2022-05-14T07:57:11.103419Z","shell.execute_reply.started":"2022-05-14T07:57:08.997149Z","shell.execute_reply":"2022-05-14T07:57:11.102546Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  \"\"\"\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Why in the fucking world is chidamabaram wasting time in the CONG party meeting ?? Discuss and decide shit what the party need to do in order to sustain in india... or just wind-up and stay at home or join with BJP..\n\nWhy in the fucking world is chidamabaram wasting time in the CONG party meeting ??\n\ndiscuss and decide to what the party need to do in order to sustain in india . or just wind - up and stay at home or join with bjp . why in the your world is chidamabaram wasting time in the cong party meeting ? ?\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  \"\"\"\n","output_type":"stream"}]},{"cell_type":"code","source":"summarized_text2 = summarize(test2)\n\ndetoxified_text2 = detoxify(test2)\n\nsummarized_detoxified_text2 = summarize(detoxified_text2)\n\nprint(test2)\nprint()\nprint(summarized_text2)\nprint()\nprint(summarized_detoxified_text2)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T07:57:11.104968Z","iopub.execute_input":"2022-05-14T07:57:11.105267Z","iopub.status.idle":"2022-05-14T07:57:12.545350Z","shell.execute_reply.started":"2022-05-14T07:57:11.105227Z","shell.execute_reply":"2022-05-14T07:57:12.544564Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  \"\"\"\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Neither of you guys has made any contribution to this Italian history article other than to shove your unhistorical unconstructive modern POV in my face. This is the reason why so many people get pissed off about the pedantry and idiocy and triviality of Wikipedia. Jesus. Get a fucking life.\n\nGet a fucking life. This is the reason why so many people get pissed off about the pedantry and idiocy and triviality of Wikipedia.\n\nneither of you guys has made any contribution to this italian history article other than to somehow your unhistorical unconstructive modern pov in my believe .\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  \"\"\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Note that, in the summarized texts post detoxification, the toxic words 'fucking' and 'shit' no longer appear!","metadata":{}}]}